/*@targets
 ** $werror $maxopt baseline
 ** sse2 sse42 xop avx2 avx512f avx512_skx
 ** vsx vsx2 vsx3
 ** neon
 **/
#define NPY_NO_EXPORT NPY_VISIBILITY_HIDDEN

#include "numpy/npy_common.h"
#include "numpy/npy_math.h" // npy_clear_floatstatus_barrier
#include "numpy/halffloat.h"
#include "simd/simd.h"
#include "loops.h"
#include "fast_loop_macros.h" // BINARY_LOOP

NPY_FINLINE int
is_overlap(char *src, npy_intp ssrc, char *dst, npy_intp sdst, int wstep)
{
    if (ssrc && sdst) {
        char *src_step1 = src + ssrc*wstep;
        char *dst_step1 = dst + sdst*wstep;
        if (dst_step1 > src_step1) {
            return (dst_step1 - src_step1) < (ssrc*wstep);
        }
        if (dst_step1 < src_step1) {
            return (src_step1 - dst_step1) < (ssrc*wstep);
        }
    }
    return 0;
}

/**begin repeat
 * #TYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONGLONG, ULONGLONG, FLOAT, DOUBLE#
 * #sfx  = s8,   u8,    s16,   u16,    s32, u32,  s64,      u64,       f32,   f64#
 * #bsfx = b8,   b8,    b16,   b16,    b32, b32,  b64,      b64,       b32,   b64#
 * #len  = 8,    8,     16,    16,     32,  32,   64,       64,        32,    64#
 * #VCHK = //*9,                                                             _F64#
 * #CLR  = 0*8,                                                        1,     1#
 */
/**begin repeat1
 * #kind   = equal, not_equal, greater, greater_equal, logical_and, logical_or#
 * #intrin = cmpeq, cmpneq,    cmpgt,   cmpge,         and,         or#
 * #OP     = ==,    !=,        >,       >=,            &&,          ||#
 * #LOGIC  = 0,     0,         0,       0,             1,           1#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    char *bsrc0 = args[0], *bsrc1 = args[1], *bdst = args[2];
    const npy_intp ssrc0 = steps[0], ssrc1 = steps[1], sdst = steps[2];
    npy_intp len = dimensions[0];
#if NPY_SIMD@VCHK@
    const int wstep = npyv_nlanes_u8;
    const int ssize = sizeof(npyv_lanetype_@sfx@);
    const int dsize = sizeof(npyv_lanetype_u8);
    const npyv_u8 v_one = npyv_setall_u8(1);
    #if @LOGIC@ != 0
    const npyv_@sfx@ v_zero = npyv_zero_@sfx@();
    #endif
    if (sizeof(npy_bool) != 1) {
        goto NO_SIMD;
    }
    // TODO: check alignment size for arm7
    if (is_overlap(bsrc0, ssrc0, bdst, sdst, wstep) || is_overlap(bsrc1, ssrc1, bdst, sdst, wstep)) {
        goto NO_SIMD;
    }
    // all contiguous
    if (ssrc0 == ssize && ssrc1 == ssrc0 && sdst == dsize) {
        for (; len >= wstep; len -= wstep, bsrc0 += ssrc0 * wstep, bsrc1 += ssrc1 * wstep,
                                           bdst  += sdst  * wstep
        ) {
            const int slanes = npyv_nlanes_@sfx@;
            const npyv_lanetype_@sfx@ *src0 = (npyv_lanetype_@sfx@*)bsrc0;
            const npyv_lanetype_@sfx@ *src1 = (npyv_lanetype_@sfx@*)bsrc1;
                  npyv_lanetype_u8    *dst  = (npyv_lanetype_u8*)bdst;
            /****************************************************************************
             ** Load
             ***************************************************************************/
            /**begin repeat2
             * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
             * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
             */
            #if @len@ > @GL@
                npyv_@sfx@ v_src0@N@ = npyv_load_@sfx@(src0 + slanes*@N@);
                npyv_@sfx@ v_src1@N@ = npyv_load_@sfx@(src1 + slanes*@N@);
            #endif
            /**end repeat2**/
            /****************************************************************************
             ** Test
             ***************************************************************************/
            /**begin repeat2
             * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
             * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
             */
            #if @len@ > @GL@ && @LOGIC@ == 1
                npyv_@bsfx@ v_mask@N@ = npyv_@intrin@_@bsfx@(
                    npyv_cmpneq_@sfx@(v_src0@N@, v_zero),
                    npyv_cmpneq_@sfx@(v_src1@N@, v_zero)
                );
            #elif @len@ > @GL@
                npyv_@bsfx@ v_mask@N@ = npyv_@intrin@_@sfx@(v_src0@N@, v_src1@N@);
            #endif
            /**end repeat2**/
            /****************************************************************************
             ** Pack
             ***************************************************************************/
            #if @len@ == 16
                npyv_b8 v_mask = npyv_pack_b16(v_mask0, v_mask1);
            #elif @len@ == 32
                npyv_b8 v_mask = npyv_pack_b8_b32(v_mask0, v_mask1, v_mask2, v_mask3);
            #elif @len@ == 64
                npyv_b8 v_mask = npyv_pack_b8_b64(
                    v_mask0, v_mask1, v_mask2, v_mask3,
                    v_mask4, v_mask5, v_mask6, v_mask7
                );
            #else
                npyv_b8 v_mask = v_mask0;
            #endif
            /****************************************************************************
             ** Store
             ***************************************************************************/
            npyv_store_u8(dst, npyv_and_u8(npyv_cvt_u8_b8(v_mask), v_one));
        } // all contiguous loop
    } // all contiguous condition
    /*
    else if (bdst == 0) {
        // TODO: reduce
        for (; len >= wstep; len -= wstep, bsrc0 += ssrc0 * wstep, bsrc1 += ssrc1 * wstep)
        {
        }
    }
    */
    else {
        // TODO: check max stride
        for (; len >= wstep; len -= wstep, bsrc0 += ssrc0 * wstep, bsrc1 += ssrc1 * wstep,
                                           bdst  += sdst  * wstep)
        {
            const int slanes = npyv_nlanes_@sfx@;
            const npyv_lanetype_@sfx@ *src0 = (npyv_lanetype_@sfx@*)bsrc0;
            const npyv_lanetype_@sfx@ *src1 = (npyv_lanetype_@sfx@*)bsrc1;
                  npyv_lanetype_u8    *dst  = (npyv_lanetype_u8*)bdst;
            /**************************************************************************
             ** Load
             **************************************************************************/
            /**begin repeat2
            * #SN = 0, 1#
            */
            /**begin repeat3
            * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
            * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
            */
            #if @len@ > @GL@
                npyv_@sfx@ v_src@SN@@N@;
            #endif
            /**end repeat3**/
            if (ssrc@SN@ == ssize) {
                /**begin repeat3
                * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
                * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
                */
                #if @len@ > @GL@
                    v_src@SN@@N@ = npyv_load_@sfx@(src@SN@ + slanes*@N@);
                #endif
                /**end repeat3**/
            }
            #if 0
            // TODO: specialize reverse
            // else if (ssrc@SN@ == -ssize) {
            }
            // TODO: specialize GBA
            else if (ssrc@SN@ == ssize*2) {
            }
            else if (ssrc@SN@ == ssize*3) {
            }
            else if (ssrc@SN@ == ssize*4) {
            }
            #endif
            else {
                const int stride@SN@ = (int)(ssrc@SN@ / sizeof(src@SN@[0]));
                /**begin repeat3
                * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
                * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
                */
                #if @len@ > @GL@
                    v_src@SN@@N@ = npyv_loadn_@sfx@(src@SN@ + stride@SN@ * @N@ * slanes, stride@SN@);
                #endif
                /**end repeat3**/
            }
            /**end repeat2**/
            /**************************************************************************
             ** Test
             **************************************************************************/
            /**begin repeat2
            * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
            * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
            */
            #if @len@ > @GL@
                npyv_@bsfx@ v_mask@N@;
            #endif
            /**end repeat2**/
            #if @LOGIC@ == 1
                if (ssrc0 == 0) {
                    const npyv_@bsfx@ v_splats = npyv_cmpneq_@sfx@(
                        npyv_setall_@sfx@(src0[0]), v_zero
                    );
                    /**begin repeat2
                     * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
                     * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
                     */
                    #if @len@ > @GL@
                        v_mask@N@ = npyv_@intrin@_@bsfx@(
                            v_splats, npyv_cmpneq_@sfx@(v_src1@N@, v_zero)
                        );
                    #endif
                    /**end repeat2**/
                }
                else if (ssrc1 == 0) {
                    const npyv_@bsfx@ v_splats = npyv_cmpneq_@sfx@(
                        npyv_setall_@sfx@(src1[0]), v_zero
                    );
                    /**begin repeat2
                     * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
                     * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
                     */
                    #if @len@ > @GL@
                        v_mask@N@ = npyv_@intrin@_@bsfx@(
                            npyv_cmpneq_@sfx@(v_src0@N@, v_zero), v_splats
                        );
                    #endif
                    /**end repeat2**/
                }
            #else // else LOGIC == 1
                if (ssrc0 == 0) {
                    const npyv_@sfx@ v_splats = npyv_setall_@sfx@(src0[0]);
                    /**begin repeat2
                     * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
                     * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
                     */
                    #if @len@ > @GL@
                        v_mask@N@ = npyv_@intrin@_@sfx@(v_splats, v_src1@N@);
                    #endif
                    /**end repeat2**/
                }
                else if (ssrc1 == 0) {
                    const npyv_@sfx@ v_splats = npyv_setall_@sfx@(src1[0]);
                    /**begin repeat2
                     * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
                     * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
                     */
                    #if @len@ > @GL@
                        v_mask@N@ = npyv_@intrin@_@sfx@(v_src0@N@, v_splats);
                    #endif
                    /**end repeat2**/
                }
            #endif // LOGIC == 1
            else {
                /**begin repeat2
                 * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
                 * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
                 */
                #if @len@ > @GL@ && @LOGIC@ == 1
                    v_mask@N@ = npyv_@intrin@_@bsfx@(
                        npyv_cmpneq_@sfx@(v_src0@N@, v_zero),
                        npyv_cmpneq_@sfx@(v_src1@N@, v_zero)
                    );
                #elif @len@ > @GL@
                    v_mask@N@ = npyv_@intrin@_@sfx@(v_src0@N@, v_src1@N@);
                #endif
                /**end repeat2**/
            }
            /**************************************************************************
             ** Pack
             **************************************************************************/
            #if @len@ == 16
                npyv_b8 v_mask = npyv_pack_b16(v_mask0, v_mask1);
            #elif @len@ == 32
                npyv_b8 v_mask = npyv_pack_b8_b32(v_mask0, v_mask1, v_mask2, v_mask3);
            #elif @len@ == 64
                npyv_b8 v_mask = npyv_pack_b8_b64(
                    v_mask0, v_mask1, v_mask2, v_mask3,
                    v_mask4, v_mask5, v_mask6, v_mask7
                );
            #else
                npyv_b8 v_mask = v_mask0;
            #endif
            /**************************************************************************
             ** Store
             **************************************************************************/
            npyv_u8 bmask = npyv_and_u8(npyv_cvt_u8_b8(v_mask), v_one);
            if (sdst == dsize) {
                npyv_store_u8(dst, bmask);
            } else {
                npyv_storen_u8(dst, (int)sdst, bmask);
            }
        }
    }
    npyv_cleanup();
NO_SIMD:
#endif // NPY_SIMD
    for (; len > 0; --len, bsrc0 += ssrc0, bsrc1 += ssrc1, bdst += sdst) {
        const npyv_lanetype_@sfx@ src0 = *((npyv_lanetype_@sfx@*)bsrc0);
        const npyv_lanetype_@sfx@ src1 = *((npyv_lanetype_@sfx@*)bsrc1);
        *((npy_bool*)bdst) = (src0 @OP@ src1);
    }
#if @CLR@
    npy_clear_floatstatus_barrier((char*)dimensions);
#endif
}
/**end repeat1**/
/**end repeat**/

/**************************************************************************
 ** Mapping
 **************************************************************************/
/**begin repeat
 * #TYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONGLONG, ULONGLONG, FLOAT, DOUBLE#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_less)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)
{
    char *nargs[3] = {args[1], args[0], args[2]};
    npy_intp nsteps[3] = {steps[1], steps[0], steps[2]};
    NPY_CPU_DISPATCH_CURFX(@TYPE@_greater)(nargs, dimensions, nsteps, func);
}
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_less_equal)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)
{
    char *nargs[3] = {args[1], args[0], args[2]};
    npy_intp nsteps[3] = {steps[1], steps[0], steps[2]};
    NPY_CPU_DISPATCH_CURFX(@TYPE@_greater_equal)(nargs, dimensions, nsteps, func);
}
/**end repeat**/

/**begin repeat
 * #kind = equal, not_equal, greater, greater_equal, less, less_equal, logical_and, logical_or#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(LONG_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)
{
    if (sizeof(npy_long) == sizeof(npy_int)) {
        NPY_CPU_DISPATCH_CURFX(INT_@kind@)(args, dimensions, steps, func);
    } else {
        NPY_CPU_DISPATCH_CURFX(LONGLONG_@kind@)(args, dimensions, steps, func);
    }
}

NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(ULONG_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)
{
    if (sizeof(npy_ulong) == sizeof(npy_uint)) {
        NPY_CPU_DISPATCH_CURFX(UINT_@kind@)(args, dimensions, steps, func);
    } else {
        NPY_CPU_DISPATCH_CURFX(ULONGLONG_@kind@)(args, dimensions, steps, func);
    }
}
/**end repeat**/

/**************************************************************************
 ** TODO: Optimize the following
 **************************************************************************/
/**begin repeat
 * #kind = equal, not_equal, greater, greater_equal, less, less_equal#
 * #OP =  ==, !=, >, >=, <, <=#
 **/

NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(BOOL_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        npy_bool in1 = *((npy_bool *)ip1) != 0;
        npy_bool in2 = *((npy_bool *)ip2) != 0;
        *((npy_bool *)op1)= in1 @OP@ in2;
    }
}
/**end repeat**/

/**begin repeat
 * #kind = logical_and, logical_or#
 * #OP =  &&, ||#
 * #SC =  ==, !=#
 * #and = 1, 0#
 **/
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(BOOL_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    if(IS_BINARY_REDUCE) {
        /* for now only use libc on 32-bit/non-x86 */
        if (steps[1] == 1) {
            npy_bool * op = (npy_bool *)args[0];
#if @and@
            /* np.all(), search for a zero (false) */
            if (*op) {
                *op = memchr(args[1], 0, dimensions[0]) == NULL;
            }
#else
            /*
             * np.any(), search for a non-zero (true) via comparing against
             * zero blocks, memcmp is faster than memchr on SSE4 machines
             * with glibc >= 2.12 and memchr can only check for equal 1
             */
            static const npy_bool zero[4096]; /* zero by C standard */
            npy_uintp i, n = dimensions[0];

            for (i = 0; !*op && i < n - (n % sizeof(zero)); i += sizeof(zero)) {
                *op = memcmp(&args[1][i], zero, sizeof(zero)) != 0;
            }
            if (!*op && n - i > 0) {
                *op = memcmp(&args[1][i], zero, n - i) != 0;
            }
#endif
            return;
        }
        else {
            BINARY_REDUCE_LOOP(npy_bool) {
                const npy_bool in2 = *(npy_bool *)ip2;
                io1 = io1 @OP@ in2;
                if (io1 @SC@ 0) {
                    break;
                }
            }
            *((npy_bool *)iop1) = io1;
        }
    }
    else {
        BINARY_LOOP {
            const npy_bool in1 = *(npy_bool *)ip1;
            const npy_bool in2 = *(npy_bool *)ip2;
            *((npy_bool *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat**/


/**begin repeat
 * #kind = equal, not_equal, less, less_equal, greater, greater_equal,
 *        logical_and, logical_or#
 * #OP = ==, !=, <, <=, >, >=, &&, ||#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(LONGDOUBLE_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const npy_longdouble in1 = *(npy_longdouble *)ip1;
        const npy_longdouble in2 = *(npy_longdouble *)ip2;
        *((npy_bool *)op1) = in1 @OP@ in2;
    }
    npy_clear_floatstatus_barrier((char*)dimensions);
}
/**end repeat**/

#define _HALF_LOGICAL_AND(a,b) (!npy_half_iszero(a) && !npy_half_iszero(b))
#define _HALF_LOGICAL_OR(a,b) (!npy_half_iszero(a) || !npy_half_iszero(b))
/**begin repeat
 * #kind = equal, not_equal, less, less_equal, greater,
 *         greater_equal, logical_and, logical_or#
 * #OP = npy_half_eq, npy_half_ne, npy_half_lt, npy_half_le, npy_half_gt,
 *       npy_half_ge, _HALF_LOGICAL_AND, _HALF_LOGICAL_OR#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(HALF_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const npy_half in1 = *(npy_half *)ip1;
        const npy_half in2 = *(npy_half *)ip2;
        *((npy_bool *)op1) = @OP@(in1, in2);
    }
}
/**end repeat**/
#undef _HALF_LOGICAL_AND
#undef _HALF_LOGICAL_OR

#define CGE(xr,xi,yr,yi) ((xr > yr && !npy_isnan(xi) && !npy_isnan(yi)) \
                          || (xr == yr && xi >= yi))
#define CLE(xr,xi,yr,yi) ((xr < yr && !npy_isnan(xi) && !npy_isnan(yi)) \
                          || (xr == yr && xi <= yi))
#define CGT(xr,xi,yr,yi) ((xr > yr && !npy_isnan(xi) && !npy_isnan(yi)) \
                          || (xr == yr && xi > yi))
#define CLT(xr,xi,yr,yi) ((xr < yr && !npy_isnan(xi) && !npy_isnan(yi)) \
                          || (xr == yr && xi < yi))
#define CEQ(xr,xi,yr,yi) (xr == yr && xi == yi)
#define CNE(xr,xi,yr,yi) (xr != yr || xi != yi)


/**begin repeat
 * complex types
 * #TYPE = CFLOAT, CDOUBLE, CLONGDOUBLE#
 * #ftype = npy_float, npy_double, npy_longdouble#
 * #c = f, , l#
 * #C = F, , L#
 */
/**begin repeat1
 * #kind= greater, greater_equal, less, less_equal, equal, not_equal#
 * #OP = CGT, CGE, CLT, CLE, CEQ, CNE#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        *((npy_bool *)op1) = @OP@(in1r,in1i,in2r,in2i);
    }
}
/**end repeat1**/

/**begin repeat1
   #kind = logical_and, logical_or#
   #OP1 = ||, ||#
   #OP2 = &&, ||#
*/
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        *((npy_bool *)op1) = (in1r @OP1@ in1i) @OP2@ (in2r @OP1@ in2i);
    }
}
/**end repeat1**/
/**end repeat**/
