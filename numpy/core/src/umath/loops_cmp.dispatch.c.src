/*@targets
 ** $werror $maxopt baseline
 ** sse2 xop avx2
 ** vsx vsx2 vsx3
 ** neon
 **/
#define NPY_NO_EXPORT NPY_VISIBILITY_HIDDEN

#include "numpy/npy_common.h"
#include "numpy/npy_math.h" // npy_clear_floatstatus_barrier
#include "numpy/halffloat.h"
#include "simd/simd.h"
#include "loops.h"
#include "fast_loop_macros.h" // BINARY_LOOP

NPY_FINLINE int
is_overlap(char *src, npy_intp ssrc, char *dst, npy_intp sdst, int wstep)
{
    if (ssrc && sdst) {
        char *src_step1 = src + ssrc*wstep;
        char *dst_step1 = dst + sdst*wstep;
        if (dst_step1 > src_step1) {
            return (dst_step1 - src_step1) < (ssrc*wstep);
        }
        if (dst_step1 < src_step1) {
            return (src_step1 - dst_step1) < (ssrc*wstep);
        }
    }
    return 0;
}

#define CONTIG  0
#define NCONTIG 1
#define SCALAR  2

/**begin repeat
 * #TYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONGLONG, ULONGLONG, FLOAT, DOUBLE#
 * #sfx  = s8,   u8,    s16,   u16,    s32, u32,  s64,      u64,       f32,   f64#
 * #bsfx = b8,   b8,    b16,   b16,    b32, b32,  b64,      b64,       b32,   b64#
 * #len  = 8,    8,     16,    16,     32,  32,   64,       64,        32,    64#
 * #VCHK = NPY_SIMD*9,                                                        NPY_SIMD_F64#
 * #CLR  = 0*8,                                                        1,     1#
 */
/**begin repeat1
 * #kind   = equal, not_equal, greater, greater_equal, logical_and, logical_or#
 * #intrin = cmpeq, cmpneq,    cmpgt,   cmpge,         and,         or#
 * #OP     = ==,    !=,        >,       >=,            &&,          ||#
 * #LOGIC  = 0,     0,         0,       0,             1,           1#
 * #SWAP   = 1,     1,         0,       0,             1,           1#
 */
/**begin repeat2
 * #S0TYPE = CONTIG, CONTIG,  NCONTIG, NCONTIG, SCALAR, SCALAR#
 * #S1TYPE = CONTIG, CONTIG,  NCONTIG, CONTIG,  CONTIG, NCONTIG#
 * #D0TYPE = CONTIG, NCONTIG, CONTIG,  CONTIG,  CONTIG, CONTIG#
 */
static void @TYPE@_@kind@_@S0TYPE@_@S1TYPE@_@D0TYPE@
(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
    const npyv_lanetype_@sfx@ *src0 = (npyv_lanetype_@sfx@*)args[0];
    const npyv_lanetype_@sfx@ *src1 = (npyv_lanetype_@sfx@*)args[1];
          npyv_lanetype_u8    *dst  = (npyv_lanetype_u8*)args[2];
                       npy_intp len = dimensions[0];
#if @S0TYPE@ == CONTIG
    const int ssrc0 = 1;
#elif @S0TYPE@ == SCALAR
    const int ssrc0 = 0;
#else
    const npy_intp ssrc0 = steps[0] / sizeof(src0[0]);
#endif
#if @S1TYPE@ == CONTIG
    const int ssrc1 = 1;
#elif @S1TYPE@ == SCALAR
    const int ssrc1 = 0;
#else
    const npy_intp ssrc1 = steps[1] / sizeof(src1[0]);
#endif
#if @D0TYPE@ == CONTIG
    const int sdst = 1;
#elif @D0TYPE@ == SCALAR
    const int sdst = 0;
#else
    const npy_intp sdst = steps[2];
#endif
#if @VCHK@
    const int wstep = npyv_nlanes_u8;
    #if @S0TYPE@ == SCALAR
        const npyv_@sfx@ v_scalar0 = npyv_setall_@sfx@(*src0);
    #endif
    #if @S1TYPE@ == SCALAR
        const npyv_@sfx@ v_scalar1 = npyv_setall_@sfx@(*src1);
    #endif
    #if @LOGIC@ != 0
        const npyv_@sfx@ v_zero = npyv_zero_@sfx@();
    #endif
    const npyv_u8 v_one = npyv_setall_u8(1);
    for (; len >= wstep; len -= wstep, src0 += wstep*ssrc0, src1 += wstep*ssrc1, dst += wstep*sdst) {
        /****************************************************************************
         ** Load
         ***************************************************************************/
        const int slanes = npyv_nlanes_@sfx@;
        /**begin repeat3
         * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
         * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
         */
        #if @len@ > @GL@
            #if @S0TYPE@ == CONTIG
                npyv_@sfx@ v_src0@N@ = npyv_load_@sfx@(src0 + slanes*@N@);
            #elif @S0TYPE@ == NCONTIG
                npyv_@sfx@ v_src0@N@ = npyv_loadn_@sfx@(src0 + ssrc0*slanes*@N@, (int)ssrc0);
            #endif
            #if @S1TYPE@ == CONTIG
                npyv_@sfx@ v_src1@N@ = npyv_load_@sfx@(src1 + slanes*@N@);
            #elif @S1TYPE@ == NCONTIG
                npyv_@sfx@ v_src1@N@ = npyv_loadn_@sfx@(src1 + ssrc1*slanes*@N@, (int)ssrc1);
            #endif
        #endif
        /**end repeat3**/
        /****************************************************************************
         ** Test
         ***************************************************************************/
        /**begin repeat3
         * #N  = 0, 1, 2,  3,  4,  5,  6,  7#
         * #GL = 0, 8, 16, 16, 32, 32, 32, 32#
         */
        #if @len@ > @GL@ && @LOGIC@ == 1
            #if @S0TYPE@ == SCALAR
                npyv_@bsfx@ v_mask@N@ = npyv_@intrin@_@bsfx@(
                    npyv_cmpneq_@sfx@(v_scalar0, v_zero),
                    npyv_cmpneq_@sfx@(v_src1@N@, v_zero)
                );
            #elif @S1TYPE@ == SCALAR
                npyv_@bsfx@ v_mask@N@ = npyv_@intrin@_@bsfx@(
                    npyv_cmpneq_@sfx@(v_src0@N@, v_zero),
                    npyv_cmpneq_@sfx@(v_scalar1, v_zero)
                );
            #else
                npyv_@bsfx@ v_mask@N@ = npyv_@intrin@_@bsfx@(
                    npyv_cmpneq_@sfx@(v_src0@N@, v_zero),
                    npyv_cmpneq_@sfx@(v_src1@N@, v_zero)
                );
            #endif
        #elif @len@ > @GL@
            #if @S0TYPE@ == SCALAR
                npyv_@bsfx@ v_mask@N@ = npyv_@intrin@_@sfx@(v_scalar0, v_src1@N@);
            #elif @S1TYPE@ == SCALAR
                npyv_@bsfx@ v_mask@N@ = npyv_@intrin@_@sfx@(v_src1@N@, v_scalar1);
            #else
                npyv_@bsfx@ v_mask@N@ = npyv_@intrin@_@sfx@(v_src0@N@, v_src1@N@);
            #endif
        #endif
        /**end repeat3**/
        /****************************************************************************
         ** Pack
         ***************************************************************************/
        #if @len@ == 16
            npyv_b8 v_mask = npyv_pack_b16(v_mask0, v_mask1);
        #elif @len@ == 32
            npyv_b8 v_mask = npyv_pack_b8_b32(v_mask0, v_mask1, v_mask2, v_mask3);
        #elif @len@ == 64
            npyv_b8 v_mask = npyv_pack_b8_b64(
                v_mask0, v_mask1, v_mask2, v_mask3,
                v_mask4, v_mask5, v_mask6, v_mask7
            );
        #else
            npyv_b8 v_mask = v_mask0;
        #endif
        /****************************************************************************
         ** Store
         ***************************************************************************/
        npyv_u8 vu_mask = npyv_and_u8(npyv_cvt_u8_b8(v_mask), v_one);
        #if @D0TYPE@ == CONTIG
            npyv_store_u8(dst, vu_mask);
        #else
            npyv_storen_u8(dst, (int)sdst, vu_mask);
        #endif
    }
    npyv_cleanup();
#elif !defined(NPY_DISABLE_OPTIMIZATION)
    for (; len >= 4; len -= 4, src0 += ssrc0*4, src1 += ssrc1*4, dst += sdst*4) {
        /**begin repeat3
         * #N  = 0, 1, 2, 3#
         */
        const npyv_lanetype_@sfx@ src0@N@ = src0[ssrc0*@N@];
        const npyv_lanetype_@sfx@ src1@N@ = src1[ssrc1*@N@];
        /**end repeat3**/
        /**begin repeat3
         * #N  = 0, 1, 2, 3#
         */
        dst[sdst*@N@] = src0@N@ @OP@ src1@N@;
        /**end repeat3**/
    }
#endif // @VCHK@
    for (; len > 0; --len, src0 += ssrc0, src1 += ssrc1, dst += sdst) {
        const npyv_lanetype_@sfx@ src00 = *src0;
        const npyv_lanetype_@sfx@ src10 = *src1;
        *dst = src00 @OP@ src10;
    }
}
/**end repeat2**/
/**end repeat1**/
/**end repeat**/

/**begin repeat
 * #TYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONGLONG, ULONGLONG, FLOAT, DOUBLE#
 * #sfx  = s8,   u8,    s16,   u16,    s32, u32,  s64,      u64,       f32,   f64#
 * #CLR  = 0*8,                                                        1,     1#
 */
/**begin repeat1
 * #kind = equal, not_equal, greater, greater_equal, logical_and, logical_or#
 * #SWAP = 1,     1,         0,       0,             1,           1#
 * #OP   = ==,    !=,        >,       >=,            &&,          ||#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    const npy_intp s0 = steps[0], s1 = steps[1], d0 = steps[2];
    const int sz = sizeof(npyv_lanetype_@sfx@);
    const int dz = sizeof(npyv_lanetype_u8);
    // TODO: check alignment size for arm7
    //const int wstep = npyv_nlanes_u8;
    /*if (is_overlap(bsrc0, ssrc0, bdst, sdst, wstep) || is_overlap(bsrc1, ssrc1, bdst, sdst, wstep)) {
        goto NO_SIMD;
    }*/
    if (d0==dz && s0==sz && s1==sz) {
        @TYPE@_@kind@_CONTIG_CONTIG_CONTIG(args, dimensions, steps);
    }
    /**begin repeat2
     * #S0TYPE = SCALAR, NCONTIG, SCALAR#
     * #S1TYPE = CONTIG,  CONTIG, NCONTIG#
     * #D0TYPE = CONTIG,  CONTIG, CONTIG#
     * #COND   = d0==dz && s0==0 && s1==sz, 
                 d0==dz &&          s1==sz,
                 d0==dz && s0==0#
     */
    else if (@COND@) {
        @TYPE@_@kind@_@S0TYPE@_@S1TYPE@_@D0TYPE@(args, dimensions, steps);
    }
    /**end repeat2**/
    #if @SWAP@
        /**begin repeat2
         * #S0TYPE = SCALAR, NCONTIG, SCALAR#
         * #S1TYPE = CONTIG,  CONTIG, NCONTIG#
         * #D0TYPE = CONTIG,  CONTIG, CONTIG#
         * #COND   = d0==dz && s0==sz && s1==0,
                     d0==dz && s0==sz,
                     d0==dz && s1==0#
         */
        else if (@COND@) {
            char *nargs[3] = {args[1], args[0], args[2]};
            npy_intp nsteps[3] = {steps[1], steps[0], steps[2]};
            @TYPE@_@kind@_@S0TYPE@_@S1TYPE@_@D0TYPE@(nargs, dimensions, nsteps);
        }
        /**end repeat2**/
    #endif // SWAP
    else if (s0==sz && s1==sz) {
        @TYPE@_@kind@_CONTIG_CONTIG_NCONTIG(args, dimensions, steps);
    }
    else if (d0==dz) {
        @TYPE@_@kind@_NCONTIG_NCONTIG_CONTIG(args, dimensions, steps);
    }    
    else {
        BINARY_LOOP {
            const npyv_lanetype_@sfx@ src00 = *((npyv_lanetype_@sfx@*)ip1);
            const npyv_lanetype_@sfx@ src10 = *((npyv_lanetype_@sfx@*)ip2);
            *((npy_bool *)op1)= src00 @OP@ src10;
        }
    }
#if @CLR@
    npy_clear_floatstatus_barrier((char*)dimensions);
#endif
}
/**end repeat1**/
/**end repeat**/

/**************************************************************************
 ** Mapping
 **************************************************************************/
/**begin repeat
 * #TYPE = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONGLONG, ULONGLONG, FLOAT, DOUBLE#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_less)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)
{
    char *nargs[3] = {args[1], args[0], args[2]};
    npy_intp nsteps[3] = {steps[1], steps[0], steps[2]};
    NPY_CPU_DISPATCH_CURFX(@TYPE@_greater)(nargs, dimensions, nsteps, func);
}
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_less_equal)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)
{
    char *nargs[3] = {args[1], args[0], args[2]};
    npy_intp nsteps[3] = {steps[1], steps[0], steps[2]};
    NPY_CPU_DISPATCH_CURFX(@TYPE@_greater_equal)(nargs, dimensions, nsteps, func);
}
/**end repeat**/

/**begin repeat
 * #kind = equal, not_equal, greater, greater_equal, less, less_equal, logical_and, logical_or#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(LONG_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)
{
    if (sizeof(npy_long) == sizeof(npy_int)) {
        NPY_CPU_DISPATCH_CURFX(INT_@kind@)(args, dimensions, steps, func);
    } else {
        NPY_CPU_DISPATCH_CURFX(LONGLONG_@kind@)(args, dimensions, steps, func);
    }
}

NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(ULONG_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *func)
{
    if (sizeof(npy_ulong) == sizeof(npy_uint)) {
        NPY_CPU_DISPATCH_CURFX(UINT_@kind@)(args, dimensions, steps, func);
    } else {
        NPY_CPU_DISPATCH_CURFX(ULONGLONG_@kind@)(args, dimensions, steps, func);
    }
}
/**end repeat**/

/**************************************************************************
 ** TODO: Optimize the following
 **************************************************************************/
/**begin repeat
 * #kind = equal, not_equal, greater, greater_equal, less, less_equal#
 * #OP =  ==, !=, >, >=, <, <=#
 **/

NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(BOOL_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{

    BINARY_LOOP {
        npy_bool in1 = *((npy_bool *)ip1) != 0;
        npy_bool in2 = *((npy_bool *)ip2) != 0;
        *((npy_bool *)op1)= in1 @OP@ in2;
    }
}
/**end repeat**/

/**begin repeat
 * #kind = logical_and, logical_or#
 * #OP =  &&, ||#
 * #SC =  ==, !=#
 * #and = 1, 0#
 **/
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(BOOL_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    if(IS_BINARY_REDUCE) {
        /* for now only use libc on 32-bit/non-x86 */
        if (steps[1] == 1) {
            npy_bool * op = (npy_bool *)args[0];
#if @and@
            /* np.all(), search for a zero (false) */
            if (*op) {
                *op = memchr(args[1], 0, dimensions[0]) == NULL;
            }
#else
            /*
             * np.any(), search for a non-zero (true) via comparing against
             * zero blocks, memcmp is faster than memchr on SSE4 machines
             * with glibc >= 2.12 and memchr can only check for equal 1
             */
            static const npy_bool zero[4096]; /* zero by C standard */
            npy_uintp i, n = dimensions[0];

            for (i = 0; !*op && i < n - (n % sizeof(zero)); i += sizeof(zero)) {
                *op = memcmp(&args[1][i], zero, sizeof(zero)) != 0;
            }
            if (!*op && n - i > 0) {
                *op = memcmp(&args[1][i], zero, n - i) != 0;
            }
#endif
            return;
        }
        else {
            BINARY_REDUCE_LOOP(npy_bool) {
                const npy_bool in2 = *(npy_bool *)ip2;
                io1 = io1 @OP@ in2;
                if (io1 @SC@ 0) {
                    break;
                }
            }
            *((npy_bool *)iop1) = io1;
        }
    }
    else {
        BINARY_LOOP {
            const npy_bool in1 = *(npy_bool *)ip1;
            const npy_bool in2 = *(npy_bool *)ip2;
            *((npy_bool *)op1) = in1 @OP@ in2;
        }
    }
}
/**end repeat**/


/**begin repeat
 * #kind = equal, not_equal, less, less_equal, greater, greater_equal,
 *        logical_and, logical_or#
 * #OP = ==, !=, <, <=, >, >=, &&, ||#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(LONGDOUBLE_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const npy_longdouble in1 = *(npy_longdouble *)ip1;
        const npy_longdouble in2 = *(npy_longdouble *)ip2;
        *((npy_bool *)op1) = in1 @OP@ in2;
    }
    npy_clear_floatstatus_barrier((char*)dimensions);
}
/**end repeat**/

#define _HALF_LOGICAL_AND(a,b) (!npy_half_iszero(a) && !npy_half_iszero(b))
#define _HALF_LOGICAL_OR(a,b) (!npy_half_iszero(a) || !npy_half_iszero(b))
/**begin repeat
 * #kind = equal, not_equal, less, less_equal, greater,
 *         greater_equal, logical_and, logical_or#
 * #OP = npy_half_eq, npy_half_ne, npy_half_lt, npy_half_le, npy_half_gt,
 *       npy_half_ge, _HALF_LOGICAL_AND, _HALF_LOGICAL_OR#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(HALF_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const npy_half in1 = *(npy_half *)ip1;
        const npy_half in2 = *(npy_half *)ip2;
        *((npy_bool *)op1) = @OP@(in1, in2);
    }
}
/**end repeat**/
#undef _HALF_LOGICAL_AND
#undef _HALF_LOGICAL_OR

#define CGE(xr,xi,yr,yi) ((xr > yr && !npy_isnan(xi) && !npy_isnan(yi)) \
                          || (xr == yr && xi >= yi))
#define CLE(xr,xi,yr,yi) ((xr < yr && !npy_isnan(xi) && !npy_isnan(yi)) \
                          || (xr == yr && xi <= yi))
#define CGT(xr,xi,yr,yi) ((xr > yr && !npy_isnan(xi) && !npy_isnan(yi)) \
                          || (xr == yr && xi > yi))
#define CLT(xr,xi,yr,yi) ((xr < yr && !npy_isnan(xi) && !npy_isnan(yi)) \
                          || (xr == yr && xi < yi))
#define CEQ(xr,xi,yr,yi) (xr == yr && xi == yi)
#define CNE(xr,xi,yr,yi) (xr != yr || xi != yi)


/**begin repeat
 * complex types
 * #TYPE = CFLOAT, CDOUBLE, CLONGDOUBLE#
 * #ftype = npy_float, npy_double, npy_longdouble#
 * #c = f, , l#
 * #C = F, , L#
 */
/**begin repeat1
 * #kind= greater, greater_equal, less, less_equal, equal, not_equal#
 * #OP = CGT, CGE, CLT, CLE, CEQ, CNE#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        *((npy_bool *)op1) = @OP@(in1r,in1i,in2r,in2i);
    }
}
/**end repeat1**/

/**begin repeat1
   #kind = logical_and, logical_or#
   #OP1 = ||, ||#
   #OP2 = &&, ||#
*/
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    BINARY_LOOP {
        const @ftype@ in1r = ((@ftype@ *)ip1)[0];
        const @ftype@ in1i = ((@ftype@ *)ip1)[1];
        const @ftype@ in2r = ((@ftype@ *)ip2)[0];
        const @ftype@ in2i = ((@ftype@ *)ip2)[1];
        *((npy_bool *)op1) = (in1r @OP1@ in1i) @OP2@ (in2r @OP1@ in2i);
    }
}
/**end repeat1**/
/**end repeat**/
